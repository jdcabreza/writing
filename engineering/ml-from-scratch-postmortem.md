---
title: "Building ML From Scratch: A Postmortem"
date_created: 2025-05-08
date_published: 2025-05-09
date_updated: 2025-05-09
tags: [machine learning, career, reflection]
draft: true
reading_time: 10m
---

# Building ML From Scratch: A Postmortem

I recently had the opportunity to help build a machine learning stack from the ground up --- no infrastructure, no established processes, and data that was far from ideal.

It was both exciting and overwhelming. I was (and still am) early on in my career, and now I've become responsible for helping productionize ML in an organization that had never done it before. There was lots to learn, both technically and interpersonally.

As it turns out though, building the actual system itself was the easy part. The hard part was everything around it --- infrastructure, data, and alignment with those involved.

This post is a reflection (a postmortem, really; but not as a way to complain or rant) about what it actually takes to get ML off of the ground.


## 1. First Things First

The obvious first thing is data. It's very well known that you can't build a model with clean data. I often refer to the [AI hierarchy of needs](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007) whenever someone effectively says "just build a model". There is often a strong desire for ML wins in an org (many of which are reasonable), but if the foundational layers aren't there then it's just not going to happen.

We ran into issues with data maturity early on, having to coordinate with other teams to figure out where data was coming from, what it meant, and how to access it. We eventually got the pipelines that we needed in place but they were stitched together as notebooks and were fragile. We'd often have bugs and we'd then have to figure out whether an issue came from our code or if it came from the underlying data. We eventually decided to keep our dependencies minimal --- we'd request raw data and then do all the transformations ourselves (probably not the best way, but we had more control at least).

Another bottleneck we had was infrastructure. It was hard to experiment with things because there was so much friction. We'd need to create a formal request via tickets to get anything spun up, even in a dev environment (plus we'd also need to justify things). On top of that, we needed to stay cloud agnostic so we couldn't exactly rely on managed services either. At the time, cost optimization was also a high priority and a lot of our architecture decisions down the line eventually almost always came down to: "can we even afford to do this?"

All of these led to slow iteration. We had gotten our first feature out the door and had a lot of momentum and motivation, but we'd constantly get blocked by things outside of our control and they eventually faded away.

**Lessons**:
- You can't "just start doing ML" without the lifeblood of ML: (clean) data.
- You can only go as fast as the limitations your environment imposes (artificial or not).
- Velocity needs momentum and motivation. Momentum and motivation need autonomy. No autonomy, no velocity.


## 2. Ship Ugly, Learn Fast

Speed matters. The faster you get something in front of users, the faster you learn what actually works. Stakeholders also want to see ROI early on --- delivering something tangible helps build credibility fast.

Our initial ML platform was crude. We didn't have any labels for our data, so we built our own system to ingest Kafka topics and produce the ground truth we needed at the time. We didn't have a formal feature or model store, so we just used a cloud bucket to store raw data and files. Our predictions were essentially generated by a Dockerized Python script wrapped in a cron job.

It was messy and definitely not the most elegant solution, but it worked end-to-end and we shipped it. This was where I learned the most about ML systems and what it really took to get them to production (in terms of technical skill, at least).

**Lessons**:
- Don't over-engineer. Keep things as simple as possible, even if it can seem unrefined.
- Feedback loops are key to improving your solutions.


## 3. Know What Impact Looks Like (or else...)

It's too easy to assume that a project will naturally lead to impact. This is especially true when leadership is excited about it and when there's a grand vision. Unfortunately, those aren't enough. Without any concrete strategy, any grandiose vision just becomes a nice story.

Despite strong support and a great vision for ML at the time, we didn't have any clear way to measure success. Business metrics were hard to access and there was no definition of "impact". Our initial objective was to "predict XYZ", and that's what we did. We shipped our first feature and it was great. Users were using it, with some even advocating for it. But, we never knew if the business actually improved. We shipped our ML system and it was the same --- no one knew if it was successful or not. It was hard to know what to do next as a result and the project ended up just silently stalling until... nothing.

**Lessons**:
- Know the impact of your project. No impact means failure, even for well-backed, well-meaning projects.
- No feedback loop, no known metrics to look at means you're essentially flying blind.
- Technical complexity does not guarantee business value.


## 4. No One's Coming to Save You

When things start going sideways, it's tempting to wait for someone to step in and just... do something. The reality is that that help is unlikely to come when you need it the most. If you're hoping a new hire will solve things, well, they'll probably need weeks (or months) to ramp up. If you're hoping someone will take ownership and steer things in a better direction, well, chances are they're already stretched thin themselves.

Our team wore every hat just to ship. We coordinated with other teams to lay down the foundations, getting the data and infrastructure we needed. We became our own product managers, figuring out what to do next based on what we knew at the time. We essentially just rolled up our sleeves and said "fine, let's do it ourselves."

**Lessons**:
- Don't assume support is coming (as bad as it sounds).
- Be proactive. If you aren't leading the work, then who else will?
- Sometimes, it's sink or swim. It's up to you to choose which it's going to be.


## At the End of the Day...

Building ML from scratch in an environment that's less-than-ideal is... hard. You can either ship something valuable and create real momentum or you work hard only to eventually hit a brick wall.

I'm sure that I grew to have better technical skills, patience, and a clearer sense of what ML entails in an organization because of these experiences and I'm grateful for that. Technical lessons ended up being pretty straightforward, but the more interpersonal and organizational ones were very much hard-earned.

At the end of the day, you can't control the hand that you get dealt. You can only ever control what you do with it. Some problems are yours to solve, and some problems are probably best dealt with by someone else (especially as an engineer without much influence) --- only you can decide if things are worth pushing through with or if it's time to take a step back and... re-evaluate.

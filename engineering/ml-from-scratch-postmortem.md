---
title: 
date_created: 2025-05-08
date_published:
date_updated: 
tags: [machine learning, career, reflection]
draft: true
reading_time: 5m
---

# Building ML From Scratch: A Postmortem

I recently had the opportunity to help build a machine learning stack from the ground up --- no infrastructure, no established processes, and data that was far from ideal.

It was both exciting and overwhelming. I was (and still am) early on in my career, and now I've become responsible for helping productionize ML in an organization that had never done it before. There was lots to learn, both technically and interpersonally.

As it turns out though, building the actual system itself was the easy part. The hard part was everything around it --- infrastructure, data, and alignment with those involved.

This post is a reflection (a postmortem, really; but not as a way to complain or rant) about what it actually takes to get ML off of the ground.

## 1. First Things First

The obvious first thing is data. It's very well known that you can't build a model with clean data. I often refer to the [AI hierarchy of needs](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007) whenever someone effectively says "just build a model". There is often a strong desire for ML wins in an org (many of which are reasonable), but if the foundational layers aren't there then it's just not going to happen.

We ran into issues with data maturity early on, having to coordinate with other teams to figure out where data was coming from, what it meant, and how to access it. We eventually got the pipelines that we needed in place but they were stitched together as notebooks and were fragile. We'd often have bugs and we'd then have to figure out whether an issue came from our code or if it came from the underlying data. We eventually decided to keep our dependencies minimal --- we'd request raw data and then do all the transformations ourselves (crude, but we had more control).

Another bottleneck we had was infrastructure. We couldn't experiment freely because of a lack of permissions and constant budget constraints. I couldn't even spin up my own cloud bucket without a formal request. It was hard to lean on any managed resources too because we wanted to stay cloud-agnostic. Later architecture decisions (when we were trying to improve our initial system) almost always came down to: "can we even afford this?"

All of these conditions led to slow experimentation. We had gotten our first feature out the door and had a lot of momentum and motivation, but we'd constantly get blocked by things outside of our control and it eventually faded away.

All of this led to slow, painful iteration. By the time we got our first feature out the door, we had a lot of momentum. We eventually constantly got blocked by things outside of our control and it eventually dwindled.

**Lessons**:
- You can't "just start doing ML" without the lifeblood of ML: (clean) data.
- You can only go as fast as the limitations your environment imposes (artificial or not).
- Velocity requires autonomy. Without autonomy, momentum and motivation will likely fade.


## 2. Ship Ugly, Learn Fast

Speed matters. The faster you get something in front of users, the faster you learn what actually works. Stakeholders also want to see ROI early on --- delivering something tangible helps build credibility fast.

Our initial ML platform was very crude. We didn't have any labels for our data, so we built our own system to ingest Kafka topics and produce the ground truth we needed at the time. We didn't have a formal feature or model store, so we just used a cloud bucket to store raw data and files. Our predictions were essentially generated by a Dockerized Python script wrapped in a cron job.

It was messy and definitely not the most elegant solution, but it worked end-to-end and we shipped it. This was where I learned the most about ML systems and what it really took to get them to production (in terms of technical skill, at least).

**Lessons**:
- Don't over-engineer. Keep things as simple as possible, even if it can seem crude.
- Feedback loops are key to improving your solutions.

## 3. Know What Impact Looks Like (or else...)

It's too easy to assume that a project will naturally lead to impact. This is especially true when leadership is excited about it and when there's a grand vision. Unfortunately, those aren't enough. Without any concrete strategy, a compelling idea is just a nice story.

Despite strong support and a great vision for ML, we didn't have any clear way to measure success. Business metrics were hard to access and there was no definition of "impact". Our initial objective was to "predict XYZ", and we did. 

We shipped our first feature and it was great. Users were using it, with some even advocating for it. But, we never know if the business actually improved. We shipped our ML system and it was the same --- no one knew if it was successful or not. It was hard to know what to do next as a result and the project ended up just silently stalling until... nothing.

**Lessons**:
- Define impact before you write a single line of code. No impact = dead project.
- Technical complexity does not guarantee business value.
- If you have no shared metrics and a feedback loop, you're flying blind.
- Even a well-backed project will die if they aren't tied to real outcomes.

## 4. No One's Coming to Save You

When things start going sideways, it's tempting to wait for someone to step in and just... do something. The reality is that help is unlikely to come when you need it the most. If you're hoping a new hire will solve things, well, need weeks or months to ramp up. If you're hoping someone will take ownership, well, chances are they're already stretched thin themslves.

Our team had to wear every hat just to ship. We coordinated with other teams just to progress, getting the data and infrastructure we needed. We became our own product managers, figuring out what to do next based on what we knew at the time. We ran our own sprints and retros. We essentially just rolled up our sleeves and said "fine, let's do it ourselves."

**Lessons**:
- Don't assume support is coming (as bad as it sounds).
- Be proactive. If you aren't leading the work, then who else will?
- Sometimes, it's sink or swim. It's up to you to choose which it's going to be.

## At the End of the Day...

Building ML from scratch in an environment that's less-than-ideal is... hard. You can either ship smething valuable and create real momentum or you work hard only to hit a wall and crash.

I'm sure that I grew to have better technical skills, patience, and a clearer sense of what ML entails in an organization. Technical lessons were straightforward, but the more interpersonal and organizational ones were much, much harder.

At the end of the day, some problems just aren't yours to solve. You can't control the hand that you were dealt, but you can control how you react to and use it.
